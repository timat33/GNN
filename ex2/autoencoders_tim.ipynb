{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_dimensions(layers:int, input_size:int, hidden_size:int, bottleneck_size:int) -> np.array:\n",
    "    \"\"\"\n",
    "    Get an np.array of layer dimensions to construct a set of linear layers. \n",
    "    Start from input_size, increase linearly in dimension up to hidden_size, then back down to bottleneck_size\n",
    "    (ie encoder), then proceed symmetrically for the decoder.\n",
    "    \"\"\"\n",
    "    # Obtain encoder layer dimensions \n",
    "    num_increasing_layers = np.floor(layers/2).astype(int)\n",
    "    num_decreasing_layers = layers - num_increasing_layers\n",
    "    increasing_layer_dims = np.linspace(input_size, hidden_size, num_increasing_layers, dtype = int)\n",
    "    decreasing_layer_dims = np.linspace(hidden_size, bottleneck_size, num_decreasing_layers, dtype = int)\n",
    "    encoder_layer_dims = np.concatenate([increasing_layer_dims,decreasing_layer_dims])\n",
    "\n",
    "    # obtain decoder layer dims symmetric to encoder and concatenate together for all layer dims\n",
    "    decoder_layer_dims = encoder_layer_dims[::-1] # Reverse and then skip first element (don't duplicate bottleneck)\n",
    "\n",
    "    return encoder_layer_dims, decoder_layer_dims\n",
    "\n",
    "def create_mlp_from_dim_array(dims: np.array) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Given an array of dimensions, create an mlp that has linear layers followed by ReLus except in the last step\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for l in range(len(dims) - 1):\n",
    "            # Add a Linear layer with the given input and output size\n",
    "            layers.append(nn.Linear(dims[l], dims[l + 1]))\n",
    "            # Add a ReLU activation function after each linear layer except after bottleneck and end\n",
    "            if l < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "\n",
    "    return model\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, bottleneck_size, hidden_size, layers):\n",
    "        \"\"\"\n",
    "        Pass hidden size either as an int (in which case ramp linearly from input size to\n",
    "        hidden size and back down to bottleneck size) or as a list of dimensions for each layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Store architecture parameters\n",
    "        self.input_size = input_size\n",
    "        self.bottleneck_size = bottleneck_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "\n",
    "        # Obtain layer dimensions if an int is passed for hidden_size\n",
    "        if isinstance(hidden_size, int):\n",
    "            encoder_dims, decoder_dims = get_dimensions(layers, input_size, hidden_size, bottleneck_size)\n",
    "        else:\n",
    "            encoder_dims = np.array(hidden_size)\n",
    "            decoder_dims = np.array(hidden_size[::-1])\n",
    "\n",
    "\n",
    "        # Make model of alternating linear layers and ReLu activations\n",
    "        self.encoder = create_mlp_from_dim_array(encoder_dims)\n",
    "        self.decoder = create_mlp_from_dim_array(decoder_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass standardised data through model\n",
    "        code = self.encoder(x)\n",
    "        x_hat = self.decoder(code)\n",
    "\n",
    "        return code, x_hat\n",
    "\n",
    "def train_epoch(model, train_loader, epoch_index, tb_writer, optimiser, loss_fn, mmd_loss = False):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    metrics = {'MAE': 0} \n",
    "\n",
    "    for i, x_batch in enumerate(train_loader):\n",
    "        # Zero out gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Make predictions (discard codes)\n",
    "        codes, x_hat = model(x_batch)\n",
    "\n",
    "        # Get loss, metrics, and gradients\n",
    "        if not mmd_loss:\n",
    "            loss = loss_fn(x_hat, x_batch)\n",
    "        else:\n",
    "            loss = loss_fn(x_hat, codes, x_batch)\n",
    "        loss.backward()\n",
    "        metrics['MAE'] += nn.functional.l1_loss(x_hat, x_batch).detach() \n",
    "\n",
    "        # Update\n",
    "        optimiser.step()\n",
    "\n",
    "        # Write to tensorboard\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print(f'  batch {i+1} training loss: {last_loss}')\n",
    "            total_batches = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, total_batches)\n",
    "            running_loss = 0.\n",
    "\n",
    "    metrics['MAE'] = metrics['MAE']/len(train_loader) # Divide by number of batches to get average MAE\n",
    "    print(f'  epoch {epoch_index+1} training MAE (batch averaged): {metrics[\"MAE\"]}')\n",
    "\n",
    "    tb_writer.flush() # Write batch losses for this epoch to disk\n",
    "    return last_loss\n",
    "\n",
    "def get_train_val_split(x_train, batch_size, seed = 11121):\n",
    "    # Make a training/validation split: shuffle and then split 80/20\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    shuffled_inds = torch.randperm(x_train.shape[0])\n",
    "    x_train = x_train[shuffled_inds]\n",
    "    cut_index = np.floor(0.8*x_train.shape[0]).astype(int)\n",
    "    x_train, x_val = x_train[:cut_index], x_train[cut_index:]\n",
    "\n",
    "    train_loader = DataLoader(x_train, batch_size = batch_size)\n",
    "    val_loader = DataLoader(x_val, batch_size=4)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def get_val_loss(model, val_loader, loss_fn, mmd_loss = False):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for x_batch in val_loader:\n",
    "        code, x_hat = model(x_batch)\n",
    "\n",
    "        if not mmd_loss:\n",
    "            loss = loss_fn(x_hat, x_batch)\n",
    "        else:\n",
    "            loss = loss_fn(x_hat, code, x_batch)\n",
    "\n",
    "        val_loss+=loss\n",
    "\n",
    "    # Divide to get loss averaged over batches\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "def train_model(model, n_epoch, loss_fn, x_train, lr, mmd_loss = False, batch_size = 4, seed = 11121):\n",
    "    # Get extra necessary objects\n",
    "    tb_writer = SummaryWriter()\n",
    "    optimiser = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "    # Define counter for early stopping to avoid overfitting/computation inefficiency\n",
    "    early_stop_counter = 0\n",
    "    early_stop_counter_max = 5 # Stop if no improvement in val loss after this many epochs\n",
    "\n",
    "    # Make a training/validation split: shuffle and then split\n",
    "    train_loader, val_loader = get_train_val_split(x_train, batch_size, seed)\n",
    "\n",
    "    # Train. Terminate early based on validation loss.\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch_index in range(n_epoch):\n",
    "        print('EPOCH {}:'.format(epoch_index + 1))\n",
    "\n",
    "        model.train()\n",
    "        _ = train_epoch(model, train_loader, epoch_index, tb_writer, optimiser, loss_fn, mmd_loss)\n",
    "\n",
    "        # Get validation loss\n",
    "        val_loss = get_val_loss(model, val_loader, loss_fn, mmd_loss)\n",
    "\n",
    "        print(f'  validation batch loss: {val_loss}')\n",
    "        total_batches = epoch_index * len(train_loader)\n",
    "        tb_writer.add_scalar('Loss/validation', val_loss, total_batches)\n",
    "\n",
    "        # If best loss is beat, then keep going. Else increment counter. Stop if counter gets too high\n",
    "        if val_loss < best_val_loss:\n",
    "            early_stop_counter = 0 # If best loss fals\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "            if early_stop_counter == early_stop_counter_max:\n",
    "                print(f'Early stopping after {epoch_index + 1} epochs')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train plain autoencoder\n",
    "# Hparams\n",
    "## Architecture hparams\n",
    "input_size = 2 # Data dimensionaliy\n",
    "bottleneck_size = 2\n",
    "hidden_size = 2 # Maximum dimension of hidden layers\n",
    "layers = 3 # number of layers in the encoder\n",
    "\n",
    "## Training hparams\n",
    "n_train = 1000\n",
    "lr = 0.001\n",
    "n_epoch = 1000\n",
    "seed = 11121\n",
    "\n",
    "# Get data\n",
    "x_train = datasets.make_moons(n_samples = n_train, noise = 0.1)[0]\n",
    "\n",
    "## convert data to tensor and standardise\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "mean, sd = x_train.mean(dim=0), x_train.std(dim=0)\n",
    "x_standardised = (x_train-mean)/sd\n",
    "\n",
    "# Get model\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "autoencoder = AutoEncoder(input_size, bottleneck_size, hidden_size, layers)\n",
    "\n",
    "# Train model\n",
    "loss_fn = nn.MSELoss()\n",
    "train_model(autoencoder, n_epoch, loss_fn, x_standardised, lr, mmd_loss = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mmd AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def squared_exponential_kernel(a, b, h = 1):\n",
    "    result = torch.empty((a.shape[0], b.shape[0]))\n",
    "\n",
    "    for i in range(a.shape[0]):\n",
    "        for j in range(b.shape[0]):\n",
    "            result[i, j] = torch.exp(-torch.sum(torch.pow(a[i]-b[j], 2))/h)\n",
    "    return result\n",
    "\n",
    "def inverse_multiquad_kernel(a, b, h = 1):\n",
    "    result = torch.empty((a.shape[0], b.shape[0]))\n",
    "    for i in range(a.shape[0]):\n",
    "        for j in range(b.shape[0]):\n",
    "            result[i, j] = torch.pow((torch.sum(torch.pow(a[i]-b[j], 2))/h+1), -1)  \n",
    "    return result\n",
    "\n",
    "def MMD2(prediction_set, true_set, bandwidth_start, num_bandwidths):\n",
    "    bandwidths = [bandwidth_start*2**i for i in range(num_bandwidths)]\n",
    "    # Define kernel functions to use: Sum of squared exponential kernels or \n",
    "    # sum of inverse multiquadratic kernels.\n",
    "    kernels = [squared_exponential_kernel, inverse_multiquad_kernel]\n",
    "\n",
    "    # Convert to np array if needed\n",
    "    if isinstance(prediction_set, list):\n",
    "        prediction_set = np.array(prediction_set)\n",
    "    if isinstance(true_set, list):\n",
    "        true_set = np.array(true_set)\n",
    "\n",
    "    # Find number of predictions and true samples\n",
    "    N = prediction_set.shape[0]\n",
    "    M = true_set.shape[0]\n",
    "\n",
    "    # Calculate squared mean discrepancy per kernel and return maximum\n",
    "    squared_mean_discrepancies = torch.zeros(len(kernels))\n",
    "    for i, kernel_unbandwidthed in enumerate(kernels):\n",
    "        kernel_squared_mean_disrepancy = 0 # will be the sum of all the bandwidthed kernels)\\\n",
    "        for bandwidth in bandwidths:\n",
    "            kernel = lambda a, b: kernel_unbandwidthed(a, b, bandwidth)\n",
    "            bandwidth_squared_mean_discrepancy = \\\n",
    "                kernel(true_set, true_set).sum() - kernel(true_set, true_set).diag().sum()/(N*(N-1)) + \\\n",
    "                kernel(prediction_set, prediction_set).sum()-kernel(prediction_set, prediction_set).diag().sum()/(M*(M-1)) - \\\n",
    "                2*kernel(prediction_set, true_set).sum()/(N*M)\n",
    "            kernel_squared_mean_disrepancy += bandwidth_squared_mean_discrepancy\n",
    "        \n",
    "        squared_mean_discrepancies[i] = kernel_squared_mean_disrepancy\n",
    "    \n",
    "    mmd2 = squared_mean_discrepancies.max()\n",
    "\n",
    "    return mmd2\n",
    "\n",
    "def mmd2_mse_loss(x_hat, code, x_batch, bandwidth_start, num_bandwidths, mmd_weight):\n",
    "    # Get MSE loss\n",
    "    mse = nn.functional.mse_loss(x_hat, x_batch)\n",
    "\n",
    "    # Get MMD2 loss between some random normal samples and the codes.\n",
    "    true_set = torch.randn(x_hat.shape)\n",
    "    mmd2 = MMD2(code, true_set, bandwidth_start, num_bandwidths)\n",
    "    \n",
    "    return mse + mmd_weight*mmd2\n",
    "\n",
    "def train_mmd_autoencoder(autoencoder, n_epoch, loss_fn, x_standardised, lr):\n",
    "    return train_model(autoencoder, n_epoch, loss_fn, x_standardised, lr, mmd_loss = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train mmd autoencoder\n",
    "# Hparams\n",
    "## Architecture hparams\n",
    "input_size = 2 # Data dimensionaliy\n",
    "bottleneck_size = 2\n",
    "hidden_size = 2 # Maximum dimension of hidden layers\n",
    "layers = 3 # number of layers in the encoder\n",
    "\n",
    "## Training hparams\n",
    "n_train = 1000\n",
    "lr = 0.001\n",
    "n_epoch = 50\n",
    "seed = 11121\n",
    "mmd_weight = 1\n",
    "bandwidth_start = 0.5\n",
    "num_bandwidths = 5\n",
    "\n",
    "# Get data\n",
    "x_train = datasets.make_moons(n_samples = n_train, noise = 0.1)[0]\n",
    "\n",
    "## convert data to tensor and standardise\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "mean, sd = x_train.mean(dim=0), x_train.std(dim=0)\n",
    "x_standardised = (x_train-mean)/sd\n",
    "\n",
    "# Get model\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "mmd_autoencoder = AutoEncoder(input_size, bottleneck_size, hidden_size, layers)\n",
    "\n",
    "# Train model\n",
    "loss_fn = partial(mmd2_mse_loss, bandwidth_start = bandwidth_start, num_bandwidths = num_bandwidths, mmd_weight = mmd_weight)\n",
    "train_mmd_autoencoder(mmd_autoencoder, n_epoch, loss_fn, x_standardised, lr)\n",
    "mmd_autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_synth_data(autoencoder, n_synth, mean, sd):\n",
    "    autoencoder.eval()\n",
    "    codes = torch.randn(n_synth, autoencoder.bottleneck_size)\n",
    "    synth_data = autoencoder.decoder(codes)\n",
    "\n",
    "    # Undo normalisation\n",
    "    synth_data = synth_data*sd + mean\n",
    "    return synth_data\n",
    "\n",
    "# Generate synthetic data\n",
    "n_synth = 10\n",
    "synth_data = gen_synth_data(autoencoder, n_synth, mean, sd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
