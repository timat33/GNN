{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for creating and training RealNVPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_digits\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data functions\n",
    "def get_standardised_moons(n, noise = 0.1, device = 'cpu'):\n",
    "    x = datasets.make_moons(n_samples = n, noise = noise)[0]\n",
    "\n",
    "    ## convert data to tensor and standardise\n",
    "    x = torch.from_numpy(x).float().to(device)\n",
    "    mean, sd = x.mean(dim=0), x.std(dim=0)\n",
    "    x_standardised = (x-mean)/sd\n",
    "\n",
    "    return x_standardised\n",
    "\n",
    "def get_standardised_gmm(n_samples, radius, device):\n",
    "    # Get vertices of regular hexagon centered at origin with radius radius\n",
    "    thetas = 2*np.pi/6 * np.arange(6)\n",
    "    vertices = np.array([np.cos(thetas), np.sin(thetas)]).reshape(6,2)\n",
    "\n",
    "    covariance_matrix = np.eye(2)*radius/10\n",
    "    covs = np.tile(covariance_matrix, (1,1,6))\n",
    "    \n",
    "    # Create GMM\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=6,\n",
    "        covariance_type='full',\n",
    "        weights_init=np.ones(6)/6  # equal weights\n",
    "    )\n",
    "    \n",
    "    # Set parameters manually\n",
    "    gmm.means_ = vertices\n",
    "    gmm.covariances_ = covs\n",
    "    gmm.weights_ = np.ones(6)/6\n",
    "    gmm.precisions_cholesky_ = np.linalg.cholesky(\n",
    "        np.linalg.inv(covs)\n",
    "    ).transpose(0, 2, 1)\n",
    "    \n",
    "    # Generate samples\n",
    "    x, _ = gmm.sample(n_samples)\n",
    "\n",
    "    ## convert data to tensor and standardise\n",
    "    x = torch.from_numpy(x).float().to(device)\n",
    "    mean, sd = x.mean(dim=0), x.std(dim=0)\n",
    "    x_standardised = (x-mean)/sd\n",
    "    \n",
    "    return x_standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising flow (INN) architecture and helper functions/classes\n",
    "def get_rand_rotation_mat(n):\n",
    "    '''\n",
    "    Obtain random rotation matrix from qr decomposition of a standard normal array\n",
    "    '''\n",
    "    a = np.random.randn(n, n)\n",
    "    q, _ = np.linalg.qr(a)\n",
    "    return q\n",
    "\n",
    "class translation_net(nn.Module):\n",
    "    def __init__(self, in_features, out_features, width):\n",
    "        '''\n",
    "        in_features: int, number of input features\n",
    "        width: int, width of the intermediate layers\n",
    "        Take input of size in_features and output a translation value as follows:\n",
    "        take input, pass through two fully connected layers with ReLU activation, \n",
    "        then pass through another fully connected layer and output a single value\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.width = width\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, width)\n",
    "        self.fc2 = nn.Linear(width, width)\n",
    "        self.fc3 = nn.Linear(width, self.out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        intermediate1 = self.relu(self.fc1(x))\n",
    "        intermediate2 = self.relu(self.fc2(intermediate1))\n",
    "        output = self.fc3(intermediate2)\n",
    "        return output\n",
    "    \n",
    "class scaling_net(nn.Module):\n",
    "    def __init__(self, in_features, out_features, width):\n",
    "        '''\n",
    "        in_features: int, number of input features\n",
    "        width: int, width of the intermediate layers\n",
    "        Same as translation block, but pass output through exp(tanh)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # Store parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.width = width\n",
    "\n",
    "        # Layers\n",
    "        self.fc1 = nn.Linear(in_features, width)\n",
    "        self.fc2 = nn.Linear(width, width)\n",
    "        self.fc3 = nn.Linear(width, self.out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        intermediate1 = self.relu(self.fc1(x))\n",
    "        intermediate2 = self.relu(self.fc2(intermediate1))\n",
    "        output_unfixed = self.fc3(intermediate2)\n",
    "        output = torch.exp(torch.tanh(output_unfixed))\n",
    "        return output\n",
    "    \n",
    "class coupling_layer(nn.Module):\n",
    "    def __init__(self, data_dim, width):\n",
    "        '''\n",
    "        Coupling block; as described in lecture\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # Store parameters\n",
    "        self.data_dim = data_dim\n",
    "        self.width = width\n",
    "        self.D_tilde = data_dim // 2 # Number of features to skip\n",
    "        self.coupling_output_dim = data_dim - self.D_tilde # same as self.D_tilde if data_dim is even\n",
    "\n",
    "        # Subnetworks\n",
    "        self.translation = translation_net(self.D_tilde, self.coupling_output_dim, width)\n",
    "        self.scaling = scaling_net(self.D_tilde, self.coupling_output_dim, width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Obtain scaling and translation coeffs\n",
    "        trans_coeff = self.translation(x[:, :self.D_tilde]) # all batches; first half\n",
    "        scale_coeff = self.scaling(x[:, :self.D_tilde]) \n",
    "\n",
    "        # Skip features up to index D_tilde, apply scaling and translation to the rest element-wise\n",
    "        output = torch.cat([x[:, :self.D_tilde], # Skip connection\n",
    "                            x[:, self.D_tilde:] * scale_coeff + trans_coeff], # Transformed\n",
    "                            dim = 1) # concatenate for each batch element\n",
    "        return output\n",
    "    \n",
    "    def reverse(self, x):\n",
    "        lower_half = x[:, self.D_tilde]\n",
    "        upper_half = x[:, self.D_tilde]\n",
    "        # Obtain scaling and translation coeffs\n",
    "        scale_coeff = self.scaling(lower_half) # [B, D-D_tilde]\n",
    "        trans_coeff = self.translation(upper_half) # [B, D-D_tilde]\n",
    "        \n",
    "\n",
    "        # Skip features up to index D_tilde, apply scaling and translation to the rest element-wise\n",
    "        output = torch.cat([lower_half, \n",
    "                            (upper_half - trans_coeff)/scale_coeff],\n",
    "                            dim = 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, blocks, device = 'cpu'):\n",
    "        '''\n",
    "        input_size: dimension of input data\n",
    "        hidden_size: width of subnetworks that determine scaling and translation\n",
    "        blocks: number of coupling layers in the model\n",
    "        \n",
    "        Proceed through `blocks` number of coupling layers with subnetworks of width `hidden_size`\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # Store parameters\n",
    "        self.data_dim = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.blocks = blocks\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Create coupling layers\n",
    "        self.coupling_layers = nn.ModuleList(\n",
    "            [coupling_layer(self.data_dim, self.hidden_size) for _ in range(self.blocks)]\n",
    "        )\n",
    "\n",
    "        # Get rotation matrices\n",
    "        self.rotation_matrices = [\n",
    "            torch.tensor(get_rand_rotation_mat(self.data_dim), \n",
    "                         requires_grad = False,\n",
    "                         dtype = torch.float32) \n",
    "            for _ in range(self.blocks-1)\n",
    "        ]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply coupling layers, interspersing with rotation matrices. Store intermediate output halves\n",
    "        output = x\n",
    "        intermediates = []\n",
    "        for i, coupling_layer in enumerate(self.coupling_layers):\n",
    "            # Store intermediate half for loss calculation\n",
    "            intermediates.append(output)\n",
    "\n",
    "            # pass through coupling layer\n",
    "            output = coupling_layer(output) \n",
    "\n",
    "            if i!= self.blocks-1:\n",
    "                # apply rotation matrix (except for last layer)\n",
    "                output = torch.einsum('ij,bj->bi', self.rotation_matrices[i], output)\n",
    "                \n",
    "        return intermediates, output\n",
    "\n",
    "    def reverse(self, x):\n",
    "        # Apply coupling layers in reverse order, interspersing with inverse rotation matrices\n",
    "        output = x\n",
    "        for i, coupling_layer in enumerate(reversed(self.coupling_layers)):\n",
    "            if i!= 0:\n",
    "                # apply inverse rotation matrix (except for first layer)\n",
    "                output = torch.einsum('ij,bj->bi', \n",
    "                                      torch.inverse(self.rotation_matrices[i]), \n",
    "                                      output)\n",
    "\n",
    "            # pass through coupling layer\n",
    "            output = coupling_layer.reverse(output)\n",
    "\n",
    "    # Inference functions\n",
    "    def get_codes(self, x, batch_size):\n",
    "        '''\n",
    "        Pass a test tensor of data points through the mode\n",
    "        '''\n",
    "        test_loader = DataLoader(x, batch_size = batch_size)\n",
    "\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch in test_loader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                output = self.forward(x_batch)\n",
    "                outputs.append(output)\n",
    "\n",
    "        z_test = torch.cat(outputs, dim = 0) \n",
    "\n",
    "        return z_test\n",
    "    \n",
    "    def get_reconstructions(self, z, batch_size):\n",
    "        reverse_loader = DataLoader(z, batch_size = batch_size)\n",
    "\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch in reverse_loader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                output = self.reverse(x_batch)\n",
    "                outputs.append(output)\n",
    "\n",
    "        x_reconstructed = torch.cat(outputs, dim = 0)\n",
    "\n",
    "        return x_reconstructed\n",
    "    \n",
    "    def sample(self, n, batch_size = 32, seed = 11121):\n",
    "        \"\"\"\n",
    "        Sample from data distribution by generating normal samples and passing through\n",
    "        the model in reverse.\n",
    "        \"\"\"\n",
    "        if seed:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        codes = torch.randn(n)\n",
    "        reconstructions = self.get_reconstructions(codes, batch_size)\n",
    "\n",
    "        return reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for training\n",
    "class NLLLoss(nn.Module):\n",
    "    def __init__(self, coupling_layers, data_dim):\n",
    "        super().__init__()\n",
    "        self.coupling_layers = coupling_layers\n",
    "        self.data_dim = data_dim\n",
    "    \n",
    "    def forward(self, intermediates, x_hat):\n",
    "        # Component corresponding to transforming the data\n",
    "        transformed_component = (x_hat**2).sum(dim=1)/2\n",
    "        \n",
    "        # Get log_det_loss component\n",
    "        log_det_component = 0\n",
    "        for i in range(len(intermediates)):\n",
    "            # Get z_{\\leq D_tilde}\n",
    "            lower_half = intermediates[i][:, :self.data_dim//2]\n",
    "            \n",
    "            # Get sum of scaling coefficients\n",
    "            scaling_sum = torch.log(self.coupling_layers[i].scaling(lower_half)).sum(dim=1)\n",
    "            log_det_component += scaling_sum\n",
    "        \n",
    "        return (transformed_component - log_det_component).mean()\n",
    "    \n",
    "def train_epoch(model: RealNVP, train_loader, optimiser, loss_fn):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "\n",
    "    for x_batch in train_loader:\n",
    "        # Move to gpu if possible\n",
    "        x_batch = x_batch.to(model.device)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Make predictions\n",
    "        intermediates, x_hat = model(x_batch)\n",
    "\n",
    "        # Get loss, metrics, and gradients\n",
    "        loss = loss_fn(intermediates, x_hat)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update\n",
    "        optimiser.step()\n",
    "\n",
    "        # Track loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Get loss averaged over batches\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def get_train_val_split(x_train, batch_size, seed = 11121):\n",
    "    # Make a training/validation split: shuffle and then split 80/20\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    shuffled_inds = torch.randperm(x_train.shape[0])\n",
    "    x_train = x_train[shuffled_inds]\n",
    "    cut_index = np.floor(0.8*x_train.shape[0]).astype(int)\n",
    "    x_train, x_val = x_train[:cut_index], x_train[cut_index:]\n",
    "\n",
    "    train_loader = DataLoader(x_train, batch_size = batch_size)\n",
    "    val_loader = DataLoader(x_val, batch_size=4)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_val_loss(model, val_loader, loss_fn):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for x_batch in val_loader:\n",
    "        # Move to gpu if possible\n",
    "        x_batch = x_batch.to(model.device)\n",
    "\n",
    "        # Make predictions\n",
    "        intermediates, x_hat = model(x_batch)\n",
    "\n",
    "        # Get loss\n",
    "        loss = loss_fn(intermediates, x_hat)\n",
    "\n",
    "        val_loss+=loss.item()\n",
    "\n",
    "    # Divide to get loss averaged over batches\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "def train_model(model, n_epoch, loss_fn, x_train, lr, batch_size = 4, best_model_path = 'best_model.pt', seed = 11121, device = 'cpu'):\n",
    "    # Get extra necessary objects\n",
    "    optimiser = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "    # Move to gpu if possible\n",
    "    x_train = x_train.to(device)\n",
    "\n",
    "    # Define counter for early stopping to avoid overfitting/computation inefficiency\n",
    "    early_stop_counter = 0\n",
    "    early_stop_counter_max = 5 # Stop if no improvement in val loss after this many epochs\n",
    "\n",
    "    # Make a training/validation split: shuffle and then split\n",
    "    train_loader, val_loader = get_train_val_split(x_train, batch_size, seed)\n",
    "\n",
    "    # Train. Terminate early based on validation loss\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch_index in range(n_epoch):\n",
    "        print('EPOCH {}:'.format(epoch_index + 1))\n",
    "\n",
    "        model.train()\n",
    "        # Train and get validation loss\n",
    "        train_loss = train_epoch(model, train_loader, optimiser, loss_fn)\n",
    "        print(f'  training batch loss: {train_loss}')\n",
    "\n",
    "        val_loss = get_val_loss(model, val_loader, loss_fn)\n",
    "        print(f'  validation batch loss: {val_loss}')\n",
    "\n",
    "        # Store losses\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        # If best loss is beat, then keep going. Else increment counter. Stop if counter gets too high\n",
    "        if val_loss < best_val_loss:\n",
    "            early_stop_counter = 0 \n",
    "            best_val_loss = val_loss\n",
    "\n",
    "            # Save model state\n",
    "            torch.save({\n",
    "                'epoch': epoch_index,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimiser.state_dict(),\n",
    "                'loss': best_val_loss,\n",
    "            }, best_model_path.replace('.pt', \n",
    "                                       f'_ntrain{len(x_train)}_nepoch{n_epoch}_lr{str(lr).replace('.',',')}.pt'))\n",
    "\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "            if early_stop_counter == early_stop_counter_max:\n",
    "                print(f'Early stopping after {epoch_index + 1} epochs')\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "def init_and_train(hparams, fixed_params, best_model_path, dataset):\n",
    "    '''wrapper for creating a model and training it given hyper parameters'''\n",
    "    # Get data\n",
    "    if dataset == 'moons':\n",
    "        x_standardised = get_standardised_moons(hparams.n_train, fixed_params.noise, fixed_params.device)\n",
    "    elif dataset == 'gmm':\n",
    "        x_standardised = get_standardised_gmm(hparams.n_train, fixed_params.noise, fixed_params.device)\n",
    "\n",
    "    # Get model\n",
    "    if fixed_params.seed is not None:\n",
    "        torch.manual_seed(fixed_params.seed)\n",
    "\n",
    "    inn = RealNVP(fixed_params.input_size, \n",
    "                  hparams.hidden_size, \n",
    "                  hparams.blocks, \n",
    "                  fixed_params.device).to(fixed_params.device)\n",
    "\n",
    "    # Train model\n",
    "    loss_fn = NLLLoss(inn.coupling_layers, fixed_params.input_size)\n",
    "    train_model(inn, hparams.n_epoch, loss_fn, x_standardised, hparams.lr, best_model_path=best_model_path, device = fixed_params.device, batch_size=fixed_params.batch_size)\n",
    "\n",
    "def init_and_train_from_grid(hparams_grid, fixed_params, best_model_path, dataset):\n",
    "    '''\n",
    "    Take grid of hyperparams and train models for all combinations\n",
    "    '''\n",
    "    # Copy \n",
    "    hparams = Namespace(**vars(hparams_grid))\n",
    "    for input_size in hparams.input_size:\n",
    "        for hidden_size in hparams.hidden_size:\n",
    "            for blocks in hparams.blocks:\n",
    "                hparams.input_size = input_size\n",
    "                hparams.hidden_size = hidden_size\n",
    "                hparams.blocks = blocks\n",
    "\n",
    "                init_and_train(hparams, fixed_params, best_model_path, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hparams\n",
    "hparams_grid = Namespace()\n",
    "fixed_params = Namespace()\n",
    "\n",
    "## Architecture hparams\n",
    "hparams_grid.hidden_size = [4,8] \n",
    "hparams_grid.blocks = [3,5,7]\n",
    "\n",
    "## Training hparams\n",
    "hparams_grid.n_train = [500,1000,2000]\n",
    "hparams_grid.lr = [0.001,0.01]\n",
    "hparams_grid.n_epoch = 100\n",
    "\n",
    "# Fixed params\n",
    "fixed_params.input_size = 2 \n",
    "fixed_params.batch_size = 32\n",
    "fixed_params.noise = 0.1\n",
    "fixed_params.seed = 11121\n",
    "fixed_params.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Apply to moons dataset\n",
    "best_model_path='moons_INN.pt'\n",
    "init_and_train_from_grid(hparams_grid, fixed_params, best_model_path, 'moons')\n",
    "\n",
    "# Apply to gmm dataset\n",
    "init_and_train_from_grid(hparams_grid, fixed_params, best_model_path, 'gmm')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
