{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m AutoEncoder(input_size, bottleneck_size, hidden_size, layers)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_standardised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UniHeidelberg/GNN/testing.py:172\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, n_epoch, x_train, lr, batch_size, seed)\u001b[0m\n\u001b[1;32m    170\u001b[0m x_train \u001b[38;5;241m=\u001b[39m x_train[shuffled_inds]\n\u001b[1;32m    171\u001b[0m cut_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;241m0.8\u001b[39m\u001b[38;5;241m*\u001b[39mx_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 172\u001b[0m x_train, x_val \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcut_index\u001b[49m\u001b[43m]\u001b[49m, x_train[cut_index:]\n\u001b[1;32m    174\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(x_train, batch_size \u001b[38;5;241m=\u001b[39m batch_size)\n\u001b[1;32m    175\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(x_val, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "# AE Helper functions\n",
    "def get_dimensions(layers:int, input_size:int, hidden_size:int, bottleneck_size:int) -> np.array:\n",
    "    \"\"\"\n",
    "    Get an np.array of layer dimensions to construct a set of linear layers. \n",
    "    Start from input_size, increase linearly in dimension up to hidden_size, then back down to bottleneck_size\n",
    "    (ie encoder), then proceed symmetrically for the decoder.\n",
    "    \"\"\"\n",
    "    # Obtain encoder layer dimensions \n",
    "    num_increasing_layers = np.floor(layers/2).astype(int)\n",
    "    num_decreasing_layers = layers - num_increasing_layers\n",
    "    increasing_layer_dims = np.linspace(input_size, hidden_size, num_increasing_layers, dtype = int)\n",
    "    decreasing_layer_dims = np.linspace(hidden_size, bottleneck_size, num_decreasing_layers, dtype = int)\n",
    "    encoder_layer_dims = np.concatenate([increasing_layer_dims,decreasing_layer_dims])\n",
    "\n",
    "    # obtain decoder layer dims symmetric to encoder and concatenate together for all layer dims\n",
    "    decoder_layer_dims = encoder_layer_dims[::-1] # Reverse and then skip first element (don't duplicate bottleneck)\n",
    "\n",
    "    return encoder_layer_dims, decoder_layer_dims\n",
    "\n",
    "def create_mlp_from_dim_array(dims: np.array) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Given an array of dimensions, create an mlp that has linear layers followed by ReLus except in the last step\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for l in range(len(dims) - 1):\n",
    "            # Add a Linear layer with the given input and output size\n",
    "            layers.append(nn.Linear(dims[l], dims[l + 1]))\n",
    "            # Add a ReLU activation function after each linear layer except after bottleneck and end\n",
    "            if l < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "\n",
    "    return model\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, bottleneck_size, hidden_size, layers):\n",
    "        \"\"\"\n",
    "        Pass hidden size either as an int (in which case ramp linearly from input size to\n",
    "        hidden size and back down to bottleneck size) or as a list of dimensions for each layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Store architecture parameters\n",
    "        self.input_size = input_size\n",
    "        self.bottleneck_size = bottleneck_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "\n",
    "        # Obtain layer dimensions if an int is passed for hidden_size\n",
    "        if isinstance(hidden_size, int):\n",
    "            encoder_dims, decoder_dims = get_dimensions(layers, input_size, hidden_size, bottleneck_size)\n",
    "        else:\n",
    "            encoder_dims = np.array(hidden_size)\n",
    "            decoder_dims = np.array(hidden_size[::-1])\n",
    "\n",
    "\n",
    "        # Make model of alternating linear layers and ReLu activations\n",
    "        self.encoder = create_mlp_from_dim_array(encoder_dims)\n",
    "        self.decoder = create_mlp_from_dim_array(decoder_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass standardised data through model\n",
    "        code = self.encoder(x)\n",
    "        x_hat = self.decoder(code)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "def train_epoch(model, train_loader, epoch_index, tb_writer, optimiser, loss_fn):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    metrics = {'MAE': 0} \n",
    "\n",
    "    for i, x_batch in enumerate(train_loader):\n",
    "        # Zero out gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Make predictions\n",
    "        x_hat = model(x_batch)\n",
    "\n",
    "        # Get loss, metrics, and gradients\n",
    "        loss = loss_fn(x_hat, x_batch)\n",
    "        loss.backward()\n",
    "        metrics['MAE'] += nn.functional.l1_loss(x_hat, x_batch).detach() \n",
    "\n",
    "        # Update\n",
    "        optimiser.step()\n",
    "\n",
    "        # Write to tensorboard\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print(f'  batch {i+1} training MSE: {last_loss}')\n",
    "            total_batches = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, total_batches)\n",
    "            running_loss = 0.\n",
    "\n",
    "    metrics['MAE'] = metrics['MAE']/len(train_loader) # Divide by number of batches to get average MAE\n",
    "    print(f'  epoch {epoch_index+1} training MAE (batch averaged): {metrics[\"MAE\"]}')\n",
    "\n",
    "    tb_writer.flush() # Write batch losses for this epoch to disk\n",
    "    return last_loss\n",
    "\n",
    "def get_train_val_split(x_train, batch_size, seed = 11121):\n",
    "    # Make a training/validation split: shuffle and then split 80/20\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    shuffled_inds = torch.randperm(x_train.shape[0])\n",
    "    x_train = x_train[shuffled_inds]\n",
    "    cut_index = np.floor(0.8*x_train.shape[0]).astype(int)\n",
    "    x_train, x_val = x_train[:cut_index], x_train[cut_index:]\n",
    "\n",
    "    train_loader = DataLoader(x_train, batch_size = batch_size)\n",
    "    val_loader = DataLoader(x_val, batch_size=4)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def get_val_loss(model, val_loader, loss_fn):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for x_batch in val_loader:\n",
    "        x_hat = model(x_batch)\n",
    "\n",
    "        loss = loss_fn(x_hat, x_batch)\n",
    "        val_loss+=loss\n",
    "\n",
    "    # Divide to get loss averaged over batches\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "def train_model(model, n_epoch, x_train, lr, batch_size = 4, seed = 11121):\n",
    "    # Get extra necessary objects\n",
    "    tb_writer = SummaryWriter()\n",
    "    optimiser = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Define counter for early stopping to avoid overfitting/computation inefficiency\n",
    "    early_stop_counter = 0\n",
    "    early_stop_counter_max = 5 # Stop if no improvement in val loss after this many epochs\n",
    "\n",
    "    # Make a training/validation split: shuffle and then split\n",
    "    train_loader, val_loader = get_train_val_split(x_train, batch_size, seed)\n",
    "\n",
    "    # Train. Terminate early based on validation loss.\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch_index in range(n_epoch):\n",
    "        print('EPOCH {}:'.format(epoch_index + 1))\n",
    "\n",
    "        model.train()\n",
    "        _ = train_epoch(model, train_loader, epoch_index, tb_writer, optimiser, loss_fn)\n",
    "\n",
    "        # Get validation loss\n",
    "        val_loss = get_val_loss(model, val_loader, loss_fn)\n",
    "\n",
    "        print(f'  validation batch loss: {val_loss}')\n",
    "        total_batches = epoch_index * len(train_loader)\n",
    "        tb_writer.add_scalar('Loss/validation', val_loss, total_batches)\n",
    "\n",
    "        # If best loss is beat, then keep going. Else increment counter. Stop if counter gets too high\n",
    "        if val_loss < best_val_loss:\n",
    "            early_stop_counter = 0 # If best loss fals\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            early_stop_counter +=1\n",
    "            if early_stop_counter == early_stop_counter_max:\n",
    "                print(f'Early stopping after {epoch_index + 1} epochs')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AE functions\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # Hparams\n",
    "    ## Architecture hparams\n",
    "    input_size = 2 # Data dimensionaliy\n",
    "    bottleneck_size = 2\n",
    "    hidden_size = 2 # Maximum dimension of hidden layers\n",
    "    layers = 3 # number of layers in the encoder\n",
    "\n",
    "    ## Training hparams\n",
    "    n_train = 1000\n",
    "    lr = 0.001\n",
    "    n_epoch = 1000\n",
    "    seed = 11121\n",
    "\n",
    "    # Get data\n",
    "    x_train = datasets.make_moons(n_samples = n_train, noise = 0.1)[0]\n",
    "\n",
    "    ## convert data to tensor and standardise\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    mean, sd = x_train.mean(dim=0), x_train.std(dim=0)\n",
    "    x_standardised = (x_train-mean)/sd\n",
    "\n",
    "    # Get model\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    autoencoder = AutoEncoder(input_size, bottleneck_size, hidden_size, layers)\n",
    "\n",
    "    # Train model\n",
    "    train_model(autoencoder, n_epoch, x_standardised, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_exponential_kernel(a, b, h = 1):\n",
    "    result = np.empty((a.shape[0], b.shape[0]))\n",
    "\n",
    "    for i in range(a.shape[0]):\n",
    "        for j in range(b.shape[0]):\n",
    "            result[i, j] = np.exp(-np.sum(np.power(a[i]-b[j], 2))/h)\n",
    "    return result\n",
    "\n",
    "def inverse_multiquad_kernel(a, b, h = 1):\n",
    "    result = np.empty((a.shape[0], b.shape[0]))\n",
    "    for i in range(a.shape[0]):\n",
    "        for j in range(b.shape[0]):\n",
    "            result[i, j] = np.power((np.sum(np.power(a[i]-b[j], 2))/h+1), -1)  \n",
    "    return result\n",
    "\n",
    "def MMD2(prediction_set, true_set):\n",
    "    # Define kernel functions to use\n",
    "    kernels = [squared_exponential_kernel, inverse_multiquad_kernel]\n",
    "\n",
    "    # Convert to np array if needed\n",
    "    if isinstance(prediction_set, list):\n",
    "        prediction_set = np.array(prediction_set)\n",
    "    if isinstance(true_set, list):\n",
    "        true_set = np.array(true_set)\n",
    "\n",
    "    # Find number of predictions and true samples\n",
    "    N = prediction_set.shape[0]\n",
    "    M = true_set.shape[0]\n",
    "\n",
    "    # Calculate squared mean discrepancy per kernel and return maximum\n",
    "    squared_mean_discrepancies = []\n",
    "    for kernel in kernels:\n",
    "        kernel_squared_mean_discrepancy = \\\n",
    "            (np.sum(kernel(true_set, true_set)) - np.sum(np.diag(kernel(true_set, true_set))))/(N*(N-1)) + \\\n",
    "            (np.sum(kernel(prediction_set, prediction_set))-np.sum(np.diag(kernel(prediction_set, prediction_set))))/(M*(M-1)) - \\\n",
    "            2*(np.sum(kernel(prediction_set, true_set)))/(N*M)\n",
    "        \n",
    "        squared_mean_discrepancies.append(kernel_squared_mean_discrepancy)\n",
    "    \n",
    "    mmd2 = squared_mean_discrepancies.max()\n",
    "\n",
    "        \n",
    "    return mmd2\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
